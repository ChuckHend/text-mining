{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "raw_train = pd.read_csv('./drugsComTrain_raw.tsv', sep='\\t')\n",
    "raw_test = pd.read_csv('./drugsComTest_raw.tsv', sep='\\t')\n",
    "print(raw_train.shape)\n",
    "print(raw_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the to for .csv requirement in assignment\n",
    "raw_train['cat'] = 'TRAIN'\n",
    "raw_test['cat'] = 'TEST'\n",
    "csv = pd.concat([raw_train, raw_test], axis=0)\n",
    "csv\\\n",
    "    .to_csv('text-mining-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty descriptions\n",
    "raw_train=raw_train[[len(x)>1 for x in raw_train['review']]]\n",
    "raw_test=raw_test[[len(x)>1 for x in raw_test['review']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['rating'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['outcome'] = [1 if x > 8 else 0 for x in raw_train['rating'] ]\n",
    "raw_test['outcome'] = [1 if x > 8 else 0 for x in raw_test['rating'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing, os, json\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model_rev = Doc2Vec(\n",
    "    documents, \n",
    "    vector_size=20, \n",
    "    window=2, \n",
    "    min_count=1, \n",
    "    workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v(reviews):\n",
    "    '''reviews should be a list of strings'''\n",
    "    import re\n",
    "    # lets only remove punctuations - stop words and numbers are relevant\n",
    "    revs = [re.sub('[^A-Za-z0-9]+', ' ', x) for x in reviews]\n",
    "    embed = [model_rev.infer_vector(list(str(x))) for x in revs]\n",
    "    return pd.DataFrame(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train = d2v(raw_train['review'])\n",
    "embed_test = d2v(raw_test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train['outcome'] = raw_train['outcome']\n",
    "embed_test['outcome'] = raw_test['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = {\n",
    "    'd2v-xgb' : XGBClassifier(max_depth=14, min_child_weight=0.1, gamma=1.5,nthread=-1),\n",
    "    'd2v-lr'  : LogisticRegression(solver='lbfgs', multi_class='ovr'),\n",
    "    'd2v-mlp' : MLPClassifier(hidden_layer_sizes=(100,50)),\n",
    "    #'knn' : KNeighborsClassifier(n_neighbors=4), # takes too long for inference\n",
    "    'd2v-dt'  : DecisionTreeClassifier(min_samples_split=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bag of models\n",
    "for model_name, model in models.items():\n",
    "    print('Fitting {}'.format(model_name))\n",
    "    model.fit(\n",
    "        embed_train.drop('outcome',axis=1).values,\n",
    "        embed_train['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer w/ each model\n",
    "results = {'ACTUAL' : embed_test['outcome'].reset_index(drop=True)}\n",
    "for model_name, model in models.items():\n",
    "    print('Predicting: {}'.format(model_name))\n",
    "    results[model_name] = model.predict(embed_test.drop('outcome',axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "for model_name, model in models.items():\n",
    "    print(\"Performance Metrics for: {}\".format(model_name))\n",
    "    print(classification_report(results['ACTUAL'], results[model_name]))\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "def clean(review):\n",
    "    import re\n",
    "    # keep only alphas\n",
    "    revs = [re.sub('[^A-Za-z]+', ' ', x) for x in review]\n",
    "    cleaned = [remove_stopwords(x) for x in revs]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test['cleaned'] = clean(raw_test['review'])\n",
    "raw_train['cleaned'] = clean(raw_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = pd.DataFrame(\n",
    "    CountVectorizer(max_features=100)\\\n",
    "    .fit_transform(raw_test['cleaned'])\\\n",
    "    .toarray()\n",
    "    )\n",
    "train_vec = pd.DataFrame(\n",
    "    CountVectorizer(max_features=100)\\\n",
    "    .fit_transform(raw_train['cleaned'])\\\n",
    "    .toarray()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec['outcome'] = raw_test['outcome']\n",
    "train_vec['outcome'] = raw_train['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'vec-xgb' : XGBClassifier(max_depth=14, min_child_weight=0.1, gamma=1.5,nthread=-1),\n",
    "    'vec-lr'  : LogisticRegression(solver='lbfgs', multi_class='ovr'),\n",
    "    'vec-mlp' : MLPClassifier(hidden_layer_sizes=(100,50)),\n",
    "    'vec-dt'  : DecisionTreeClassifier(min_samples_split=2)\n",
    "}\n",
    "for model_name, model in models.items():\n",
    "    print('Fitting {}'.format(model_name))\n",
    "    model.fit(\n",
    "        train_vec.drop('outcome',axis=1).values,\n",
    "        train_vec['outcome'])\n",
    "# infer w/ each model\n",
    "# append count vec results to d2v results\n",
    "for model_name, model in models.items():\n",
    "    print('Predicting: {}'.format(model_name))\n",
    "    results[model_name] = model.predict(test_vec.drop('outcome',axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute f1-score for all models\n",
    "from sklearn.metrics import f1_score\n",
    "mods = list(results.keys())\n",
    "mods.remove('ACTUAL')\n",
    "metrics = {}\n",
    "for mod in mods:\n",
    "    f1 = f1_score(results['ACTUAL'], results[mod])\n",
    "    metrics[mod] = round(f1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame\\\n",
    "    .from_dict(metrics, orient='index')\\\n",
    "    .to_csv('f1-score.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)\\\n",
    "    .to_csv('pred.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
