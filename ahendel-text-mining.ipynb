{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report will analyze two approaches to text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: http://localhost:8888/?token=858b3d9f0fa67838d04a899b00e532c7c0b92f3a73ec6357\n",
    "\n",
    "\n",
    "The dataset provides patient reviews on specific drugs along with related conditions and a 10 star patient rating reflecting overall patient satisfaction. The data was obtained by crawling online pharmaceutical review sites. The intention was to study \n",
    "(1) sentiment analysis of drug experience over multiple facets, i.e. sentiments learned on specific aspects such as effectiveness and side effects, \n",
    "(2) the transferability of models among domains, i.e. conditions, and \n",
    "(3) the transferability of models among different data sources (see 'Drug Review Dataset (Druglib.com)'). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose is to compare the performance of text classificaiton approaches, using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(161297, 7)\n",
      "(53766, 7)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "raw_train = pd.read_csv('./drugsComTrain_raw.tsv', sep='\\t')\n",
    "raw_test = pd.read_csv('./drugsComTest_raw.tsv', sep='\\t')\n",
    "print(raw_train.shape)\n",
    "print(raw_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty descriptions\n",
    "raw_train=raw_train[[len(x)>1 for x in raw_train['review']]]\n",
    "raw_test=raw_test[[len(x)>1 for x in raw_test['review']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcbb257d9e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train['rating'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['outcome'] = [1 if x > 8 else 0 for x in raw_train['rating'] ]\n",
    "raw_test['outcome'] = [1 if x > 8 else 0 for x in raw_test['rating'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing, os, json\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model_rev = Doc2Vec(\n",
    "    documents, \n",
    "    vector_size=20, \n",
    "    window=2, \n",
    "    min_count=1, \n",
    "    workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v(reviews):\n",
    "    '''reviews should be a list of strings'''\n",
    "    import re\n",
    "    # lets only remove punctuations - stop words and numbers are relevant\n",
    "    revs = [re.sub('[^A-Za-z0-9]+', ' ', x) for x in reviews]\n",
    "    embed = [model_rev.infer_vector(list(str(x))) for x in revs]\n",
    "    return pd.DataFrame(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train = d2v(raw_train['review'])\n",
    "embed_test = d2v(raw_test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train['outcome'] = raw_train['outcome']\n",
    "embed_test['outcome'] = raw_test['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = {\n",
    "    'd2v-xgb' : XGBClassifier(max_depth=14, min_child_weight=0.1, gamma=1.5,nthread=-1),\n",
    "    'd2v-lr'  : LogisticRegression(solver='lbfgs', multi_class='ovr'),\n",
    "    'd2v-mlp' : MLPClassifier(hidden_layer_sizes=(100,50)),\n",
    "    #'knn' : KNeighborsClassifier(n_neighbors=4), # takes too long for inference\n",
    "    'd2v-dt'  : DecisionTreeClassifier(min_samples_split=2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting d2v-xgb\n",
      "Fitting d2v-lr\n",
      "Fitting d2v-mlp\n",
      "Fitting d2v-dt\n"
     ]
    }
   ],
   "source": [
    "# train bag of models\n",
    "for model_name, model in models.items():\n",
    "    print('Fitting {}'.format(model_name))\n",
    "    model.fit(\n",
    "        embed_train.drop('outcome',axis=1).values,\n",
    "        embed_train['outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: d2v-xgb\n",
      "Predicting: d2v-lr\n",
      "Predicting: d2v-mlp\n",
      "Predicting: d2v-dt\n"
     ]
    }
   ],
   "source": [
    "# infer w/ each model\n",
    "results = {'ACTUAL' : embed_test['outcome'].reset_index(drop=True)}\n",
    "for model_name, model in models.items():\n",
    "    print('Predicting: {}'.format(model_name))\n",
    "    results[model_name] = model.predict(embed_test.drop('outcome',axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics for: d2v-xgb\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75     27573\n",
      "           1       0.76      0.67      0.71     26193\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     53766\n",
      "   macro avg       0.74      0.73      0.73     53766\n",
      "weighted avg       0.74      0.74      0.73     53766\n",
      "\n",
      "-----------------\n",
      "Performance Metrics for: d2v-lr\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.98      0.67     27573\n",
      "           1       0.48      0.02      0.03     26193\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     53766\n",
      "   macro avg       0.49      0.50      0.35     53766\n",
      "weighted avg       0.49      0.51      0.36     53766\n",
      "\n",
      "-----------------\n",
      "Performance Metrics for: d2v-mlp\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.69      0.59     27573\n",
      "           1       0.49      0.31      0.38     26193\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     53766\n",
      "   macro avg       0.50      0.50      0.49     53766\n",
      "weighted avg       0.50      0.51      0.49     53766\n",
      "\n",
      "-----------------\n",
      "Performance Metrics for: d2v-dt\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80     27573\n",
      "           1       0.79      0.79      0.79     26193\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     53766\n",
      "   macro avg       0.80      0.80      0.80     53766\n",
      "weighted avg       0.80      0.80      0.80     53766\n",
      "\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "for model_name, model in models.items():\n",
    "    print(\"Performance Metrics for: {}\".format(model_name))\n",
    "    print(classification_report(results['ACTUAL'], results[model_name]))\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "def clean(review):\n",
    "    import re\n",
    "    # keep only alphas\n",
    "    revs = [re.sub('[^A-Za-z]+', ' ', x) for x in review]\n",
    "    cleaned = [remove_stopwords(x) for x in revs]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test['cleaned'] = clean(raw_test['review'])\n",
    "raw_train['cleaned'] = clean(raw_train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = pd.DataFrame(\n",
    "    CountVectorizer(max_features=100)\\\n",
    "    .fit_transform(raw_test['cleaned'])\\\n",
    "    .toarray()\n",
    "    )\n",
    "train_vec = pd.DataFrame(\n",
    "    CountVectorizer(max_features=100)\\\n",
    "    .fit_transform(raw_train['cleaned'])\\\n",
    "    .toarray()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec['outcome'] = raw_test['outcome']\n",
    "train_vec['outcome'] = raw_train['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting vec-xgb\n",
      "Fitting vec-lr\n",
      "Fitting vec-mlp\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'vec-xgb' : XGBClassifier(max_depth=14, min_child_weight=0.1, gamma=1.5,nthread=-1),\n",
    "    'vec-lr'  : LogisticRegression(solver='lbfgs', multi_class='ovr'),\n",
    "    'vec-mlp' : MLPClassifier(hidden_layer_sizes=(100,50)),\n",
    "    'vec-dt'  : DecisionTreeClassifier(min_samples_split=2)\n",
    "}\n",
    "for model_name, model in models.items():\n",
    "    print('Fitting {}'.format(model_name))\n",
    "    model.fit(\n",
    "        train_vec.drop('outcome',axis=1).values,\n",
    "        train_vec['outcome'])\n",
    "# infer w/ each model\n",
    "# append count vec results to d2v results\n",
    "for model_name, model in models.items():\n",
    "    print('Predicting: {}'.format(model_name))\n",
    "    results[model_name] = model.predict(test_vec.drop('outcome',axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute f1-score for all models\n",
    "from sklearn.metrics import f1_score\n",
    "mods = list(results.keys())\n",
    "mods.remove('ACTUAL')\n",
    "metrics = {}\n",
    "for mod in mods:\n",
    "    f1 = f1_score(results['ACTUAL'], results[mod])\n",
    "    metrics[mod] = round(f1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
