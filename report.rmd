---
title: "text-mining"
author: "Adam Hendel"
date: "April 17, 2019"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

The report explores two approaches to document vectorization, which is the transformation of text sentences to numeric features. Namely, the Bag of Words approach and Paragraph Vector (Q. Le, T. Mikolov 2014).

Both methods are popular and powerful text mining tools. However, there is no clear answer as to which method is optimal. One important difference between the two approaches is that the Paragraph Vector retains information as to the sequence of the words in a sentence. Consider the following two setnences:

1) The quick brown fox jumped over the lazy dogs.

2) The quick brown dogs jumped over the lazy fox.

These two sentences contain the same words, but with the words dog and fox swapped. The sentences are describing different events. Under the Bag of Words model, these two sentences would result in precicely the same vector. Under Paragraph Vector, there will be different.

Throughout the remainder of this report we will take a practical approach through applying both of these methods to a sample set of text data, discuss the methodology and review the results.

# Dataset Description 

The dataset is provided by Surya Kallumadi and Flex Grasser via the UCI Machine Learning Repository. It is a collection of roughly 215 thousand patient reviews of prescription drugs. The dataset is split into a 75%/25% train/test natively. Each patient review provides the name of the drug, the name of the condition, the raw text of the patient review, a numerical rating provided by the patient (10 point scale) along with the date and the number of users who found the review useful.

For this analysis, we are focusing on the textual review and the numeric rating provided by the patient. First, let us inspect the distribution of the ratings. By viewing the frequency distribution, we can see that there are many 10 point reviews and less around a score of 5. 

```{r, echo=F, warning=F, message=F}
require(ggplot2)
d <- read.csv('./drugsComTrain_raw.tsv', sep='\t')
ggplot(d,aes(x=rating))+
  geom_histogram() +
  theme_minimal()


```

Let's put the ratings into two categories; excellent and standard. All ratings with a value of 8.0 and higher will be excellent, and the rest will be standard. We can use a word cloud to get a feel for the types of words associated with higher and lower reviews.

```{r, echo=F, warning=F, message=F}
require(wordcloud)
require(tm) 
require(dplyr)

require(SnowballC) 

require(gridExtra)

high_corpus <- Corpus(VectorSource(d$review[d$rating>=8]))
low_corpus  <- Corpus(VectorSource(d$review[d$rating<8]))

high_clean <-high_corpus %>%
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace) %>%
  tm_map(removeWords, c(stopwords("english")))
  
low_clean <- low_corpus %>%
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace) %>%
  tm_map(removeWords, c(stopwords("english")))

# wordcloud(high_clean, max.words = 30, scale = c(2.5, 0.2))
```


```{r, echo=F, message=F, warning=F}
# grid.arrange(
#   wordcloud(high_clean, max.words = 30, scale = c(2.5, 0.2)),
#   wordcloud(low_clean, max.words = 30, scale = c(2.5, .5)),
#   ncol=2
# )
wordcloud(high_clean, max.words = 30, scale = c(2.5, 0.2))
wordcloud(low_clean, max.words = 30, scale = c(2.5, .45))
# wordcloud(low_clean, max.words = 30, scale = c(2.5, .5))

```

By glancing at the word clouds of the top 30 words, we can get a feel for the types of words that show up in the patient reviews. However, it is not obvious which words have a stronger association with lower reviews (right), or higher reviews (left).


# Methodology 

To explore these two vectorization methods, we will train models to predict whether the review is a standard rating (<8) or an excellent rating (>=8).

#### Data Preparation

Both of these approaches were tested using python implementations. 

For the Bag of Words approach, the Scikit-Learn Count Vectorizer implementation was used. Prior to creating the word matrix, non-numeric characters, punctuations and stop words were removed. 

The doc2vec implementation of Paragraph Vector in the Gensim package (Rehurek, R., & Sojka, P. 2010) was also used. Punctuations were removed but stop words and numeric values were not.

#### Model Fit

The following four classification models were fit to both vectorization methods:

- XGBoost (xgb)
- Multilayer Perceptron (mlp)
- Logistic Regression (lr)
- Decision Tree (dt)

The 75% training partition was used to train these models.

#### Model Inference

The 25% holdout set, for both Bag of Words and Paragraph Vector went through the same data preparation steps as previously mentioned. Each model was called on the unseen data.

Implementation and hyperparameters can be found in the appendix.


#### Results

There are several options for evaluating model performance in classification problems. For example, precision, recall, area under curve, etc. Without a use-case, no single metric is better than another. Therefore, we will use the F1 score as a well-rounded indication of the model's ability to perform on new data.

```{r, echo=F, message=F, warning=F}
require(tidyr)
results <- read.csv('./f1-score.csv')
names(results) <- c('X','F1.Score')
results <- results %>% separate(X, c('vec','model'), sep="([\\-])")
results

ggplot(results, aes(x=model, y = F1.Score, fill=vec)) +
  geom_bar(stat='identity', width=.7, position = 'dodge') +
  scale_fill_brewer('Vector Method',palette = 'Set1',
                    labels=c('Paragraph Vector', 'Bag of Words')) +
  theme_minimal()
```

Results were close in the decision tree and XGBoost models, but quite different in the multilayer perceptron and logistic regression. The Bag of Words approach did out perform Paragraph Vector in each of these model tests.

However, there are significant limitations to these findings. Several are outlined below.

Data Preparation:

Both methods would benefit from experimentation with the inclusion/removal of stop words and numeric values. There is no guarantee that either method would result in improved results.

The lengths of vectors used in this experiment was 20 for Paragraph Vector and 100 for Bag of Words. There is no relationship between the size of these vectors between models, but each can be treated as a hyperparameter to the modeling process. Thus, it would be wise to experiment with a wide range of vector lengths for both Bag of Words and Paragraph Vectors.

Model Fitting:

While the same model parameters were used for Bag of Words and Paragraph Vectors, these should be experimented with in the same fashion as the vector length noted above.

# References

Pedregosa et al. (2011), Scikit-learn: Machine Learning in Python, JMLR 12, pp. 2825-2830.

Q. Le, T. Mikolov. (2014). Distributed Represenations of Sentences and Documents. In Proceedings of ICML.

Rehurek, R., & Sojka, P. (2010). Software framework for topic modelling with large corpora. LREC.


# Appendix
```
# coding: utf-8

# Source: https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29

import pandas as pd

# load data
raw_train = pd.read_csv('./drugsComTrain_raw.tsv', sep='\t')
raw_test = pd.read_csv('./drugsComTest_raw.tsv', sep='\t')
print(raw_train.shape)
print(raw_test.shape)

# combine the to for .csv requirement in assignment
raw_train['cat'] = 'TRAIN'
raw_test['cat'] = 'TEST'
csv = pd.concat([raw_train, raw_test], axis=0)
csv\
    .to_csv('text-mining-data.csv')

# drop empty descriptions
raw_train=raw_train[[len(x)>1 for x in raw_train['review']]]
raw_test=raw_test[[len(x)>1 for x in raw_test['review']]]

raw_train['rating'].hist()

raw_train['outcome'] = [1 if x > 8 else 0 for x in raw_train['rating'] ]
raw_test['outcome'] = [1 if x > 8 else 0 for x in raw_test['rating'] ]

from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import pandas as pd
import numpy as np
import multiprocessing, os, json
cores = multiprocessing.cpu_count()

documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]
model_rev = Doc2Vec(
    documents, 
    vector_size=20, 
    window=2, 
    min_count=1, 
    workers=cores)

def d2v(reviews):
    '''reviews should be a list of strings'''
    import re
    # lets only remove punctuations - stop words and numbers are relevant
    revs = [re.sub('[^A-Za-z0-9]+', ' ', x) for x in reviews]
    embed = [model_rev.infer_vector(list(str(x))) for x in revs]
    return pd.DataFrame(embed)

embed_train = d2v(raw_train['review'])
embed_test = d2v(raw_test['review'])

embed_train['outcome'] = raw_train['outcome']
embed_test['outcome'] = raw_test['outcome']

from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

models = {
    'd2v-xgb' : XGBClassifier(max_depth=14, min_child_weight=0.1, gamma=1.5,nthread=-1),
    'd2v-lr'  : LogisticRegression(solver='lbfgs', multi_class='ovr'),
    'd2v-mlp' : MLPClassifier(hidden_layer_sizes=(100,50)),
    #'knn' : KNeighborsClassifier(n_neighbors=4), # takes too long for inference
    'd2v-dt'  : DecisionTreeClassifier(min_samples_split=2)
}

# train bag of models
for model_name, model in models.items():
    print('Fitting {}'.format(model_name))
    model.fit(
        embed_train.drop('outcome',axis=1).values,
        embed_train['outcome'])

# infer w/ each model
results = {'ACTUAL' : embed_test['outcome'].reset_index(drop=True)}
for model_name, model in models.items():
    print('Predicting: {}'.format(model_name))
    results[model_name] = model.predict(embed_test.drop('outcome',axis=1).values)

from sklearn.metrics import classification_report
for model_name, model in models.items():
    print("Performance Metrics for: {}".format(model_name))
    print(classification_report(results['ACTUAL'], results[model_name]))
    print("-----------------")

# # Word Vectorizer

from sklearn.feature_extraction.text import CountVectorizer
from gensim.parsing.preprocessing import remove_stopwords
def clean(review):
    import re
    # keep only alphas
    revs = [re.sub('[^A-Za-z]+', ' ', x) for x in review]
    cleaned = [remove_stopwords(x) for x in revs]
    return cleaned

raw_test['cleaned'] = clean(raw_test['review'])
raw_train['cleaned'] = clean(raw_train['review'])

test_vec = pd.DataFrame(
    CountVectorizer(max_features=100)\
    .fit_transform(raw_test['cleaned'])\
    .toarray()
    )
train_vec = pd.DataFrame(
    CountVectorizer(max_features=100)\
    .fit_transform(raw_train['cleaned'])\
    .toarray()
    )

test_vec['outcome'] = raw_test['outcome']
train_vec['outcome'] = raw_train['outcome']

models = {
    'vec-xgb' : XGBClassifier(max_depth=14, min_child_weight=0.1, gamma=1.5,nthread=-1),
    'vec-lr'  : LogisticRegression(solver='lbfgs', multi_class='ovr'),
    'vec-mlp' : MLPClassifier(hidden_layer_sizes=(100,50)),
    'vec-dt'  : DecisionTreeClassifier(min_samples_split=2)
}
for model_name, model in models.items():
    print('Fitting {}'.format(model_name))
    model.fit(
        train_vec.drop('outcome',axis=1).values,
        train_vec['outcome'])
# infer w/ each model
# append count vec results to d2v results
for model_name, model in models.items():
    print('Predicting: {}'.format(model_name))
    results[model_name] = model.predict(test_vec.drop('outcome',axis=1).values)

# compute f1-score for all models
from sklearn.metrics import f1_score
mods = list(results.keys())
mods.remove('ACTUAL')
metrics = {}
for mod in mods:
    f1 = f1_score(results['ACTUAL'], results[mod])
    metrics[mod] = round(f1, 3)

pd.DataFrame\
    .from_dict(metrics, orient='index')\
    .to_csv('f1-score.csv', index=True)

pd.DataFrame(results)\
    .to_csv('pred.csv', index=None)

```
